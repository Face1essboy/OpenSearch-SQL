# OpenSearch-SQL 项目服务器需求分析

## 项目概述

OpenSearch-SQL 是一个 Text-to-SQL 框架，在 BIRD 基准测试中取得了第一名。该项目通过多个模块（预处理、提取、生成、精炼、对齐）完成自然语言到 SQL 的转换。

## 一、计算资源需求

### 1.1 CPU 需求

**最低配置：**
- **核心数：** 4 核
- **推荐配置：** 8-16 核
- **原因：**
  - 项目使用多线程并发处理（ThreadPoolExecutor，默认 n=21）
  - BERT 嵌入模型在 CPU 上运行（默认配置）
  - SQLite 数据库查询操作
  - 数据处理和 JSON 解析

**关键代码位置：**
```62:62:src/pipeline/align_correct.py
    vote,none_case=muti_process_sql(Dcheck,SQLs_dic,L_values,values,question,new_db_info,hint,key_col_des,tmp_prompt,db_col,foreign_set,config['align_methods'],db_sqlite_path,n=config['n'])
```

```673:673:src/runner/check_and_correct.py
    with ThreadPoolExecutor(max_workers=n) as executor:
```

### 1.2 内存需求

**最低配置：**
- **内存：** 8 GB
- **推荐配置：** 16-32 GB
- **原因：**
  - BERT 嵌入模型加载（如 all-mpnet-base-v2, bge 等，约 400-500 MB）
  - 嵌入向量缓存（每个数据库的嵌入向量存储在内存中）
  - 多个并发线程同时处理（n=21 个线程）
  - 数据集加载（JSON 文件，如 bird_dev.json 约 10,763 行）
  - SQLite 数据库查询结果缓存

**关键代码位置：**
```80:80:src/database_process/make_emb.py
    bert_model = SentenceTransformer(bertmodel, device=device, cache_folder='model/')
```

```23:23:src/pipeline/column_retrieve_and_other_info.py
    bert_model = SentenceTransformer(config["bert_model"], device=config["device"])
```

### 1.3 GPU 需求（可选）

**GPU 使用场景：**
- **可选配置：** 如果使用本地 LLM 模型（sft_req 类）
- **推荐配置：** NVIDIA GPU，至少 16GB 显存（用于 70B 模型）
- **默认情况：** 项目主要使用 API 调用，不需要 GPU

**关键代码位置：**
```189:200:src/llm/model.py
        self.device = "cuda:0"
        self.tokenizer = AutoTokenizer.from_pretrained(
            "",
            trust_remote_code=True,
            padding_side="right",
            use_fast=True)
        self.tokenizer.pad_token = self.tokenizer.eos_token = "<|EOT|>"
        # drop device_map if running on CPU
        self.model = AutoModelForCausalLM.from_pretrained(
            "",
            torch_dtype=torch.bfloat16,
            device_map=self.device).eval()
```

**注意：** 项目默认配置中 BERT 模型使用 CPU：
```77:77:run/run_main.sh
        "device":"cpu"
```

## 二、存储需求

### 2.1 磁盘空间

**最低配置：**
- **存储：** 50 GB
- **推荐配置：** 100-200 GB
- **存储内容：**
  - 数据集文件（Bird 数据集）
  - 嵌入向量文件（.pkl.gz 格式，每个数据库一个）
  - SQLite 数据库文件
  - 模型缓存（BERT 模型，约 500 MB - 2 GB）
  - 结果文件（JSON 格式）
  - 日志文件

**关键代码位置：**
```59:61:src/database_process/make_emb.py
    with gzip.open(os.path.join(emb_dir, f'{dbname}.pkl.gz'),
                   'wb') as pkl_file:
        pickle.dump(dicts, pkl_file, protocol=pickle.HIGHEST_PROTOCOL)
```

### 2.2 存储类型

- **推荐：** SSD（用于快速读取嵌入向量和数据库查询）
- **最低：** HDD（但会影响性能）

## 三、网络需求

### 3.1 网络带宽

**最低配置：**
- **带宽：** 10 Mbps
- **推荐配置：** 50-100 Mbps
- **原因：**
  - 需要频繁调用 LLM API（OpenAI, Anthropic, Google, DeepSeek, DashScope 等）
  - 每个查询可能调用多次 API（生成候选 SQL、对齐、纠错等）
  - 默认 n=21，意味着每个查询会生成 21 个候选 SQL，每个都需要 API 调用

**支持的 API 服务：**
```8:16:src/llm/model.py
def model_chose(step,model="gpt-4 32K"):
    if model.startswith("gpt") or model.startswith("claude35_sonnet") or model.startswith("gemini"):
        return gpt_req(step,model)
    if model == "deepseek":
        return deep_seek(model)
    if model.startswith("qwen"):
        return qwenmax(model)
    if model.startswith("sft"):
        return sft_req()
```

### 3.2 API 访问要求

- **必需：** 能够访问以下 API 服务：
  - OpenAI API（GPT-4, GPT-3.5）
  - Anthropic API（Claude）
  - Google API（Gemini）
  - DeepSeek API
  - DashScope API（通义千问）
- **API Key：** 需要在代码中配置相应的 API Key

**关键代码位置：**
```22:22:run/run_main.sh
AK='your_ak' #set your ak in src/llm/model.py
```

## 四、软件环境需求

### 4.1 操作系统

- **推荐：** Linux（Ubuntu 20.04+ 或 CentOS 7+）
- **支持：** macOS, Windows（需要相应调整）

### 4.2 Python 环境

- **Python 版本：** Python 3.7+
- **推荐：** Python 3.8-3.10

### 4.3 主要依赖包

根据 `requirements.txt`：
```1:8:requirements.txt
sentence_transformers
dashscope
func_timeout
torch
pandas
numpy
chardet
openai
```

**关键依赖说明：**
- `sentence_transformers`: BERT 嵌入模型（约 500 MB - 2 GB）
- `torch`: PyTorch（如果使用 GPU，需要 CUDA 版本）
- `pandas`, `numpy`: 数据处理
- `openai`, `dashscope`: LLM API 调用

## 五、性能配置建议

### 5.1 并发配置

**默认配置：**
- `candidate_generate`: n=21（生成 21 个候选 SQL）
- `align_correct`: n=21（21 个线程并行处理）
- `NUM_WORKERS`: 3（主任务处理，但代码中已注释，实际串行）

**性能优化建议：**
- 根据服务器 CPU 核心数调整 `n` 值
- 如果内存不足，可以降低 `n` 值（如 n=10）
- 如果 API 有速率限制，需要降低并发数

**关键配置位置：**
```97:97:run/run_main.sh
        "n":21,
```

```103:103:run/run_main.sh
        "n":21,
```

### 5.2 设备配置

**BERT 模型设备：**
- **默认：** CPU（`device="cpu"`）
- **可选：** GPU（`device="cuda:0"`），如果使用 GPU 可以加速嵌入计算

**关键配置位置：**
```77:77:run/run_main.sh
        "device":"cpu"
```

## 六、服务器配置推荐

### 6.1 最低配置（开发/测试）

- **CPU：** 4 核
- **内存：** 8 GB
- **存储：** 50 GB SSD
- **网络：** 10 Mbps
- **GPU：** 不需要
- **适用场景：** 小规模测试，处理少量查询

### 6.2 推荐配置（生产环境）

- **CPU：** 8-16 核
- **内存：** 16-32 GB
- **存储：** 100-200 GB SSD
- **网络：** 50-100 Mbps
- **GPU：** 可选（如果使用本地 LLM 模型）
- **适用场景：** 处理完整数据集，批量处理

### 6.3 高性能配置（大规模处理）

- **CPU：** 16+ 核
- **内存：** 32-64 GB
- **存储：** 200+ GB SSD
- **网络：** 100+ Mbps
- **GPU：** NVIDIA GPU（16GB+ 显存，如果使用本地模型）
- **适用场景：** 大规模数据处理，高并发场景

## 七、成本考虑

### 7.1 API 调用成本

项目大量使用 LLM API，成本主要来自：
- **GPT-4o:** 约 $0.042/1K prompt tokens + $0.126/1K completion tokens
- **GPT-3.5:** 相对便宜
- **Claude, Gemini:** 根据各自定价

**每个查询的 API 调用次数：**
- 生成 DB Schema: 1 次
- 提取列值: 1 次
- 提取查询名词: 1 次
- 列检索: 1 次
- 候选生成: 1 次（但 n=21，可能产生多次调用）
- 对齐纠错: n=21 次（每个候选 SQL 一次）

**总成本估算：** 每个查询可能需要 25-50 次 API 调用，成本取决于选择的模型。

### 7.2 服务器成本

- **云服务器（最低配置）：** 约 $20-50/月
- **云服务器（推荐配置）：** 约 $50-150/月
- **本地服务器：** 一次性投资，无月费

## 八、部署建议

### 8.1 开发环境

- 使用本地机器或小型云服务器
- 降低 `n` 值以减少资源消耗
- 使用较便宜的 API 模型（如 GPT-3.5）

### 8.2 生产环境

- 使用专用服务器或高性能云实例
- 配置 API 密钥和网络访问
- 设置监控和日志系统
- 考虑使用缓存减少重复计算

### 8.3 大规模部署

- 考虑使用分布式处理
- 实现任务队列系统
- 使用数据库连接池
- 优化嵌入向量存储和检索

## 九、总结

OpenSearch-SQL 项目对服务器的要求主要集中在：

1. **CPU 和内存：** 需要足够的计算资源处理并发任务和嵌入模型
2. **网络：** 需要稳定的网络连接访问 LLM API
3. **存储：** 需要足够的空间存储数据集和嵌入向量
4. **API 成本：** 主要成本来自 LLM API 调用

**推荐配置：** 8-16 核 CPU，16-32 GB 内存，100 GB SSD，50-100 Mbps 网络带宽，无需 GPU（除非使用本地模型）。

